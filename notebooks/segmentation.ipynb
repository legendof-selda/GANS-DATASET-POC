{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install h5py==2.10.0\r\n",
    "!pip install keras==2.1.0\r\n",
    "%tensorflow_version 1.x"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UoAlafYGpsJ1",
    "outputId": "8b3eb015-67c8-46e0-b9df-b9570560acb5"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%cd /content/Mask_RCNN"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NM7u4THj5wgG",
    "outputId": "df22a9f8-07b6-4795-94d9-ae83694ab1d7"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\r\n",
    "import sys\r\n",
    "import json\r\n",
    "import datetime\r\n",
    "import numpy as np\r\n",
    "import skimage.draw\r\n",
    "import cv2\r\n",
    "from mrcnn.visualize import display_instances\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "# Root directory of the project\r\n",
    "ROOT_DIR = os.path.abspath(\"/content/wastedata-Mask_RCNN-multiple-classes/main/Mask_RCNN/\")\r\n",
    "\r\n",
    "# Import Mask RCNN\r\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\r\n",
    "from mrcnn.config import Config\r\n",
    "from mrcnn import model as modellib, utils\r\n",
    "\r\n",
    "# Path to trained weights file\r\n",
    "COCO_WEIGHTS_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\r\n",
    "\r\n",
    "# Directory to save logs and model checkpoints, if not provided\r\n",
    "# through the command line argument --logs\r\n",
    "DEFAULT_LOGS_DIR = os.path.join(ROOT_DIR, \"logs\")"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ovgSwpg3hsea",
    "outputId": "d633c4c1-77dc-4e7d-c205-48ca2aa86c25"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class CustomConfig(Config):\r\n",
    "    \"\"\"Configuration for training on the toy  dataset.\r\n",
    "    Derives from the base Config class and overrides some values.\r\n",
    "    \"\"\"\r\n",
    "    # Give the configuration a recognizable name\r\n",
    "    NAME = \"object\"\r\n",
    "\r\n",
    "    # We use a GPU with 12GB memory, which can fit two images.\r\n",
    "    # Adjust down if you use a smaller GPU.\r\n",
    "    IMAGES_PER_GPU = 2\r\n",
    "\r\n",
    "    # Number of classes (including background)\r\n",
    "    NUM_CLASSES = 1 + 6  # Background + objects\r\n",
    "\r\n",
    "    # Number of training steps per epoch\r\n",
    "    STEPS_PER_EPOCH = 40\r\n",
    "\r\n",
    "    # Skip detections with < 90% confidence\r\n",
    "    DETECTION_MIN_CONFIDENCE = 0.9"
   ],
   "outputs": [],
   "metadata": {
    "id": "plbvJ9-ujTeg"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class CustomDataset(utils.Dataset):\r\n",
    "\r\n",
    "    def load_custom(self, dataset_dir, subset):\r\n",
    "        \"\"\"Load a subset of the Dog-Cat dataset.\r\n",
    "        dataset_dir: Root directory of the dataset.\r\n",
    "        subset: Subset to load: train or val\r\n",
    "        \"\"\"\r\n",
    "        # Add classes. We have only one class to add.\r\n",
    "        self.add_class(\"object\", 1, \"building\")\r\n",
    "        self.add_class(\"object\", 2, \"tree\")\r\n",
    "        self.add_class(\"object\", 3, \"road\")\r\n",
    "        self.add_class(\"object\", 4, \"grass\")\r\n",
    "        self.add_class(\"object\", 5, \"flag\")\r\n",
    "        self.add_class(\"object\", 6, \"car\")\r\n",
    "        # self.add_class(\"object\", 3, \"xyz\")\r\n",
    "\r\n",
    "        # Train or validation dataset?\r\n",
    "        assert subset in [\"train\", \"val\"]\r\n",
    "        dataset_dir = os.path.join(dataset_dir, subset)\r\n",
    "\r\n",
    "        # Load annotations\r\n",
    "        # VGG Image Annotator saves each image in the form:\r\n",
    "        # { 'filename': '28503151_5b5b7ec140_b.jpg',\r\n",
    "        #   'regions': {\r\n",
    "        #       '0': {\r\n",
    "        #           'region_attributes': {},\r\n",
    "        #           'shape_attributes': {\r\n",
    "        #               'all_points_x': [...],\r\n",
    "        #               'all_points_y': [...],\r\n",
    "        #               'name': 'polygon'}},\r\n",
    "        #       ... more regions ...\r\n",
    "        #   },\r\n",
    "        #   'size': 100202\r\n",
    "        # }\r\n",
    "        # We mostly care about the x and y coordinates of each region\r\n",
    "        annotations1 = json.load(open(os.path.join(dataset_dir, \"WHO_N.json\")))\r\n",
    "        # print(annotations1)\r\n",
    "        annotations = list(annotations1.values())  # don't need the dict keys\r\n",
    "\r\n",
    "        # The VIA tool saves images in the JSON even if they don't have any\r\n",
    "        # annotations. Skip unannotated images.\r\n",
    "        annotations = [a for a in annotations if a['regions']]\r\n",
    "        \r\n",
    "        # Add images\r\n",
    "        for a in annotations:\r\n",
    "            # Get the x, y coordinaets of points of the polygons that make up\r\n",
    "            # the outline of each object instance. There are stores in the\r\n",
    "            # shape_attributes (see json format above)\r\n",
    "            # polygons = [r['shape_attributes'] for r in a['regions']] \r\n",
    "            # objects = [s['region_attributes']['label'] for s in a['regions']]\r\n",
    "            # print(\"objects:\",objects)\r\n",
    "            if type(a[\"regions\"]) is not list:\r\n",
    "              region=list(a[\"regions\"].values())\r\n",
    "              # Get the x, y coordinaets of points of the polygons that make up\r\n",
    "              # the outline of each object instance. There are stores in the\r\n",
    "              # shape_attributes (see json format above)\r\n",
    "              polygons = [r['shape_attributes'] for r in region] \r\n",
    "              objects = [(s['region_attributes']['label']).lower().strip() for s in region]\r\n",
    "              print(\"objects:\",objects)\r\n",
    "            else:\r\n",
    "              polygons = [r['shape_attributes'] for r in a['regions']] \r\n",
    "              objects = [s['region_attributes']['label'] for s in a['regions']]\r\n",
    "              print(\"objects:\",objects)\r\n",
    "            name_dict = {'building':1,'tree':2,'road':3,'grass':4,'flag':5,'car':6}#,\"xyz\": 3}\r\n",
    "            # key = tuple(name_dict)\r\n",
    "            num_ids = [name_dict[a] for a in objects if a]\r\n",
    "     \r\n",
    "            # num_ids = [int(n['Event']) for n in objects]\r\n",
    "            # load_mask() needs the image size to convert polygons to masks.\r\n",
    "            # Unfortunately, VIA doesn't include it in JSON, so we must read\r\n",
    "            # the image. This is only managable since the dataset is tiny.\r\n",
    "            print(\"numids\",num_ids)\r\n",
    "            image_path = os.path.join(dataset_dir, a['filename'])\r\n",
    "            image = skimage.io.imread(image_path)\r\n",
    "            height, width = image.shape[:2]\r\n",
    "\r\n",
    "            self.add_image(\r\n",
    "                \"object\",  ## for a single class just add the name here\r\n",
    "                image_id=a['filename'],  # use file name as a unique image id\r\n",
    "                path=image_path,\r\n",
    "                width=width, height=height,\r\n",
    "                polygons=polygons,\r\n",
    "                num_ids=num_ids\r\n",
    "                )\r\n",
    "\r\n",
    "    def load_mask(self, image_id):\r\n",
    "        \"\"\"Generate instance masks for an image.\r\n",
    "       Returns:\r\n",
    "        masks: A bool array of shape [height, width, instance count] with\r\n",
    "            one mask per instance.\r\n",
    "        class_ids: a 1D array of class IDs of the instance masks.\r\n",
    "        \"\"\"\r\n",
    "        # If not a Dog-Cat dataset image, delegate to parent class.\r\n",
    "        image_info = self.image_info[image_id]\r\n",
    "        if image_info[\"source\"] != \"object\":\r\n",
    "            return super(self.__class__, self).load_mask(image_id)\r\n",
    "\r\n",
    "        # Convert polygons to a bitmap mask of shape\r\n",
    "        # [height, width, instance_count]\r\n",
    "        info = self.image_info[image_id]\r\n",
    "        if info[\"source\"] != \"object\":\r\n",
    "            return super(self.__class__, self).load_mask(image_id)\r\n",
    "        num_ids = info['num_ids']\r\n",
    "        mask = np.zeros([info[\"height\"], info[\"width\"], len(info[\"polygons\"])],\r\n",
    "                        dtype=np.uint8)\r\n",
    "        for i, p in enumerate(info[\"polygons\"]):\r\n",
    "            # Get indexes of pixels inside the polygon and set them to 1\r\n",
    "        \trr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])\r\n",
    "\r\n",
    "        \tmask[rr, cc, i] = 1\r\n",
    "\r\n",
    "        # Return mask, and array of class IDs of each instance. Since we have\r\n",
    "        # one class ID only, we return an array of 1s\r\n",
    "        # Map class names to class IDs.\r\n",
    "        num_ids = np.array(num_ids, dtype=np.int32)\r\n",
    "        return mask, num_ids #np.ones([mask.shape[-1]], dtype=np.int32)\r\n",
    "\r\n",
    "    def image_reference(self, image_id):\r\n",
    "        \"\"\"Return the path of the image.\"\"\"\r\n",
    "        info = self.image_info[image_id]\r\n",
    "        if info[\"source\"] == \"object\":\r\n",
    "            return info[\"path\"]\r\n",
    "        else:\r\n",
    "            super(self.__class__, self).image_reference(image_id)"
   ],
   "outputs": [],
   "metadata": {
    "id": "Va-AE4S1kBd7"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def train(model):\r\n",
    "    \"\"\"Train the model.\"\"\"\r\n",
    "    # Training dataset.\r\n",
    "    dataset_train = CustomDataset()\r\n",
    "    dataset_train.load_custom(\"/content/drive/MyDrive/dataset2\", \"train\")\r\n",
    "    dataset_train.prepare()\r\n",
    "\r\n",
    "    # Validation dataset\r\n",
    "    dataset_val = CustomDataset()\r\n",
    "    dataset_val.load_custom(\"/content/drive/MyDrive/dataset2\", \"val\")\r\n",
    "    dataset_val.prepare()\r\n",
    "\r\n",
    "    # *** This training schedule is an example. Update to your needs ***\r\n",
    "    # Since we're using a very small dataset, and starting from\r\n",
    "    # COCO trained weights, we don't need to train too long. Also,\r\n",
    "    # no need to train all layers, just the heads should do it.\r\n",
    "    print(\"Training network heads\")\r\n",
    "    model.train(dataset_train, dataset_val,\r\n",
    "                learning_rate=config.LEARNING_RATE,\r\n",
    "                epochs=10,\r\n",
    "                layers='heads')"
   ],
   "outputs": [],
   "metadata": {
    "id": "hUzmsqLokFSO"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "config = CustomConfig()\r\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config, model_dir=DEFAULT_LOGS_DIR)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xX6xEg_IkNLz",
    "outputId": "3a8c2dfa-75e6-449c-d037-4bbb6727b61d"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "weights_path = COCO_WEIGHTS_PATH\r\n",
    "\r\n",
    "# Download weights file\r\n",
    "if not os.path.exists(weights_path):\r\n",
    "  utils.download_trained_weights(weights_path)"
   ],
   "outputs": [],
   "metadata": {
    "id": "kNCcGgm9msev"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.load_weights(weights_path, by_name=True, exclude=[\r\n",
    "            \"mrcnn_class_logits\", \"mrcnn_bbox_fc\",\r\n",
    "            \"mrcnn_bbox\", \"mrcnn_mask\"])"
   ],
   "outputs": [],
   "metadata": {
    "id": "S-hUa4BGqnaa"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#!pip install keras==2.2.5\r\n",
    "# if error occurs restart runtime\r\n",
    "train(model)"
   ],
   "outputs": [],
   "metadata": {
    "id": "7i7f9KCOueKi"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import shutil\r\n",
    "shutil.copytree('/content/wastedata-Mask_RCNN-multiple-classes/main/Mask_RCNN/logs', '/content/drive/MyDrive/MaskRCNN/2021_5_8_1025')"
   ],
   "outputs": [],
   "metadata": {
    "id": "A3A1__vPw0eB"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **TESTING**\n"
   ],
   "metadata": {
    "id": "ER2OFIbzFrhw"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\r\n",
    "import sys\r\n",
    "import random\r\n",
    "import math\r\n",
    "import re\r\n",
    "import time\r\n",
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "print(tf.__version__)\r\n",
    "import matplotlib\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import matplotlib.patches as patches\r\n",
    "import matplotlib.image as mpimg\r\n",
    "\r\n",
    "# Root directory of the project\r\n",
    "#ROOT_DIR = os.path.abspath(\"/\")\r\n",
    "\r\n",
    "# Import Mask RCNN\r\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\r\n",
    "from mrcnn import utils\r\n",
    "from mrcnn import visualize\r\n",
    "from mrcnn.visualize import display_images\r\n",
    "import mrcnn.model as modellib\r\n",
    "from mrcnn.model import log\r\n",
    "\r\n",
    "%matplotlib inline \r\n",
    "\r\n",
    "# Directory to save logs and trained model\r\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\r\n",
    "\r\n",
    "# Path to trained weights\r\n",
    "# You can download this file from the Releases page\r\n",
    "# https://github.com/matterport/Mask_RCNN/releases\r\n",
    "WEIGHTS_PATH = \"/content/wastedata-Mask_RCNN-multiple-classes/main/Mask_RCNN/logs/object20201225T0709/mask_rcnn_object_0010.h5\"  # TODO: update this path"
   ],
   "outputs": [],
   "metadata": {
    "id": "Sqi1GzS0_muj"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "config = CustomConfig()\r\n",
    "CUSTOM_DIR = os.path.join(ROOT_DIR, \"/content/dataset/\")\r\n",
    "class InferenceConfig(config.__class__):\r\n",
    "    # Run detection on one image at a time\r\n",
    "    GPU_COUNT = 1\r\n",
    "    IMAGES_PER_GPU = 1\r\n",
    "    DETECTION_MIN_CONFIDENCE = 0.7\r\n",
    "\r\n",
    "config = InferenceConfig()\r\n",
    "config.display()"
   ],
   "outputs": [],
   "metadata": {
    "id": "Z9ATL4H1GEXZ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Device to load the neural network on.\r\n",
    "# Useful if you're training a model on the same \r\n",
    "# machine, in which case use CPU and leave the\r\n",
    "# GPU for training.\r\n",
    "DEVICE = \"/gpu:0\"  # /cpu:0 or /gpu:0\r\n",
    "\r\n",
    "# Inspect the model in training or inference modes\r\n",
    "# values: 'inference' or 'training'\r\n",
    "# TODO: code for 'training' test mode not ready yet\r\n",
    "TEST_MODE = \"inference\""
   ],
   "outputs": [],
   "metadata": {
    "id": "a-F9MUrCGO7_"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_ax(rows=1, cols=1, size=16):\r\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\r\n",
    "    all visualizations in the notebook. Provide a\r\n",
    "    central point to control graph sizes.\r\n",
    "    \r\n",
    "    Adjust the size attribute to control how big to render images\r\n",
    "    \"\"\"\r\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\r\n",
    "    return ax"
   ],
   "outputs": [],
   "metadata": {
    "id": "tR1Vbd1aGje7"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load validation dataset\r\n",
    "CUSTOM_DIR = \"/content/drive/MyDrive/dataset2\"\r\n",
    "dataset = CustomDataset()\r\n",
    "dataset.load_custom(CUSTOM_DIR, \"val\")\r\n",
    "\r\n",
    "# Must call before using the dataset\r\n",
    "dataset.prepare()\r\n",
    "\r\n",
    "print(\"Images: {}\\nClasses: {}\".format(len(dataset.image_ids), dataset.class_names))"
   ],
   "outputs": [],
   "metadata": {
    "id": "jSdKUXTJGnlo"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#LOAD MODEL\r\n",
    "# Create model in inference mode\r\n",
    "with tf.device(DEVICE):\r\n",
    "    model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR,\r\n",
    "                              config=config)"
   ],
   "outputs": [],
   "metadata": {
    "id": "jMnXZbrwGvia"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load COCO weights, Or load the last model you trained\r\n",
    "weights_path = \"/content/wastedata-Mask_RCNN-multiple-classes/main/Mask_RCNN/logs/object20210805T0835/mask_rcnn_final.h5\"\r\n",
    "\r\n",
    "# Load weights\r\n",
    "print(\"Loading weights \", weights_path)\r\n",
    "model.load_weights(weights_path, by_name=True)"
   ],
   "outputs": [],
   "metadata": {
    "id": "_2EBlHj8G3-E"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# This is for predicting images which are not present in dataset\r\n",
    "#image_id = random.choice(dataset.image_ids)\r\n",
    "image1 = mpimg.imread(\"/content/drive/MyDrive/dataset2/train/11.jpg\")\r\n",
    "\r\n",
    "# Run object detection\r\n",
    "print(len([image1]))\r\n",
    "results1 = model.detect([image1], verbose=1)\r\n",
    "\r\n",
    "# Display results\r\n",
    "ax = get_ax(1)\r\n",
    "r1 = results1[0]\r\n",
    "visualize.display_instances(image1, r1['rois'], r1['masks'], r1['class_ids'],\r\n",
    "                            dataset.class_names, r1['scores'], ax=ax,\r\n",
    "                            title=\"Predictions1\")"
   ],
   "outputs": [],
   "metadata": {
    "id": "hSMyCNUyUHRp"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#RUN DETECTION\r\n",
    "image_id = random.choice(dataset.image_ids)\r\n",
    "print(image_id)\r\n",
    "image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\r\n",
    "    modellib.load_image_gt(dataset, config, image_id, use_mini_mask=False)\r\n",
    "info = dataset.image_info[image_id]\r\n",
    "print(\"image ID: {}.{} ({}) {}\".format(info[\"source\"], info[\"id\"], image_id, \r\n",
    "                                       dataset.image_reference(image_id)))\r\n",
    "\r\n",
    "# Run object detection\r\n",
    "results = model.detect([image], verbose=1)\r\n",
    "\r\n",
    "# Display results\r\n",
    "ax = get_ax(1)\r\n",
    "r = results[0]\r\n",
    "visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], \r\n",
    "                            dataset.class_names, r['scores'], ax=ax,\r\n",
    "                            title=\"Predictions\")\r\n",
    "log(\"gt_class_id\", gt_class_id)\r\n",
    "log(\"gt_bbox\", gt_bbox)\r\n",
    "log(\"gt_mask\", gt_mask)"
   ],
   "outputs": [],
   "metadata": {
    "id": "6w2AgocWHNQ5"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def color_splash(image, mask):\r\n",
    "    \"\"\"Apply color splash effect.\r\n",
    "    image: RGB image [height, width, 3]\r\n",
    "    mask: instance segmentation mask [height, width, instance count]\r\n",
    "    Returns result image.\r\n",
    "    \"\"\"\r\n",
    "    # Make a grayscale copy of the image. The grayscale copy still\r\n",
    "    # has 3 RGB channels, though.\r\n",
    "    gray = skimage.color.gray2rgb(skimage.color.rgb2gray(image)) * 255\r\n",
    "    # We're treating all instances as one, so collapse the mask into one layer\r\n",
    "    mask = (np.sum(mask, -1, keepdims=True) >= 1)\r\n",
    "    # Copy color pixels from the original color image where mask is set\r\n",
    "    if mask.shape[0] > 0:\r\n",
    "        splash = np.where(mask, image, gray).astype(np.uint8)\r\n",
    "    else:\r\n",
    "        splash = gray\r\n",
    "    return splash\r\n",
    "\r\n",
    "\r\n",
    "def detect_and_color_splash(model, image_path=None, video_path=None):\r\n",
    "    assert image_path or video_path\r\n",
    "\r\n",
    "    # Image or video?\r\n",
    "    if image_path:\r\n",
    "        # Run model detection and generate the color splash effect\r\n",
    "        print(\"Running on {}\".format(args.image))\r\n",
    "        # Read image\r\n",
    "        image = skimage.io.imread(args.image)\r\n",
    "        # Detect objects\r\n",
    "        r = model.detect([image], verbose=1)[0]\r\n",
    "        # Color splash\r\n",
    "        splash = color_splash(image, r['masks'])\r\n",
    "        # Save output\r\n",
    "        file_name = \"splash_{:%Y%m%dT%H%M%S}.png\".format(datetime.datetime.now())\r\n",
    "        skimage.io.imsave(file_name, splash)\r\n",
    "    elif video_path:\r\n",
    "        import cv2\r\n",
    "        # Video capture\r\n",
    "        vcapture = cv2.VideoCapture(video_path)\r\n",
    "        width = int(vcapture.get(cv2.CAP_PROP_FRAME_WIDTH))\r\n",
    "        height = int(vcapture.get(cv2.CAP_PROP_FRAME_HEIGHT))\r\n",
    "        fps = vcapture.get(cv2.CAP_PROP_FPS)\r\n",
    "\r\n",
    "        # Define codec and create video writer\r\n",
    "        file_name = \"splash_{:%Y%m%dT%H%M%S}.avi\".format(datetime.datetime.now())\r\n",
    "        vwriter = cv2.VideoWriter(file_name,\r\n",
    "                                  cv2.VideoWriter_fourcc(*'MJPG'),\r\n",
    "                                  fps, (width, height))\r\n",
    "\r\n",
    "        count = 0\r\n",
    "        success = True\r\n",
    "        while success:\r\n",
    "            print(\"frame: \", count)\r\n",
    "            # Read next image\r\n",
    "            success, image = vcapture.read()\r\n",
    "            if success:\r\n",
    "                # OpenCV returns images as BGR, convert to RGB\r\n",
    "                image = image[..., ::-1]\r\n",
    "                # Detect objects\r\n",
    "                r = model.detect([image], verbose=0)[0]\r\n",
    "                # Color splash\r\n",
    "                splash = color_splash(image, r['masks'])\r\n",
    "                # RGB -> BGR to save image to video\r\n",
    "                splash = splash[..., ::-1]\r\n",
    "                # Add image to video writer\r\n",
    "                vwriter.write(splash)\r\n",
    "                count += 1\r\n",
    "        vwriter.release()\r\n",
    "    print(\"Saved to \", file_name)"
   ],
   "outputs": [],
   "metadata": {
    "id": "l2Lj3jnj0g3j"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "splash = color_splash(image, r['masks'])\r\n",
    "display_images([splash], cols=1)"
   ],
   "outputs": [],
   "metadata": {
    "id": "lFtgeWLVPnxe"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# a RPN"
   ],
   "metadata": {
    "id": "4ANrZQQVRtdA"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Generate RPN trainig targets\r\n",
    "# target_rpn_match is 1 for positive anchors, -1 for negative anchors\r\n",
    "# and 0 for neutral anchors.\r\n",
    "target_rpn_match, target_rpn_bbox = modellib.build_rpn_targets(\r\n",
    "    image.shape, model.anchors, gt_class_id, gt_bbox, model.config)\r\n",
    "log(\"target_rpn_match\", target_rpn_match)\r\n",
    "log(\"target_rpn_bbox\", target_rpn_bbox)\r\n",
    "\r\n",
    "positive_anchor_ix = np.where(target_rpn_match[:] == 1)[0]\r\n",
    "negative_anchor_ix = np.where(target_rpn_match[:] == -1)[0]\r\n",
    "neutral_anchor_ix = np.where(target_rpn_match[:] == 0)[0]\r\n",
    "positive_anchors = model.anchors[positive_anchor_ix]\r\n",
    "negative_anchors = model.anchors[negative_anchor_ix]\r\n",
    "neutral_anchors = model.anchors[neutral_anchor_ix]\r\n",
    "log(\"positive_anchors\", positive_anchors)\r\n",
    "log(\"negative_anchors\", negative_anchors)\r\n",
    "log(\"neutral anchors\", neutral_anchors)\r\n",
    "\r\n",
    "# Apply refinement deltas to positive anchors\r\n",
    "refined_anchors = utils.apply_box_deltas(\r\n",
    "    positive_anchors,\r\n",
    "    target_rpn_bbox[:positive_anchors.shape[0]] * model.config.RPN_BBOX_STD_DEV)\r\n",
    "log(\"refined_anchors\", refined_anchors, )"
   ],
   "outputs": [],
   "metadata": {
    "id": "_8d9YK4dRB99"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Display positive anchors before refinement (dotted) and\r\n",
    "# after refinement (solid).\r\n",
    "visualize.draw_boxes(image, boxes=positive_anchors, refined_boxes=refined_anchors, ax=get_ax())"
   ],
   "outputs": [],
   "metadata": {
    "id": "1WSkOQSXR3NL"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# b RPN"
   ],
   "metadata": {
    "id": "XwD8dV63SCOE"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Run RPN sub-graph\r\n",
    "pillar = model.keras_model.get_layer(\"ROI\").output  # node to start searching from\r\n",
    "\r\n",
    "# TF 1.4 and 1.9 introduce new versions of NMS. Search for all names to support TF 1.3~1.10\r\n",
    "nms_node = model.ancestor(pillar, \"ROI/rpn_non_max_suppression:0\")\r\n",
    "if nms_node is None:\r\n",
    "    nms_node = model.ancestor(pillar, \"ROI/rpn_non_max_suppression/NonMaxSuppressionV2:0\")\r\n",
    "if nms_node is None: #TF 1.9-1.10\r\n",
    "    nms_node = model.ancestor(pillar, \"ROI/rpn_non_max_suppression/NonMaxSuppressionV3:0\")\r\n",
    "\r\n",
    "rpn = model.run_graph([image], [\r\n",
    "    (\"rpn_class\", model.keras_model.get_layer(\"rpn_class\").output),\r\n",
    "    (\"pre_nms_anchors\", model.ancestor(pillar, \"ROI/pre_nms_anchors:0\")),\r\n",
    "    (\"refined_anchors\", model.ancestor(pillar, \"ROI/refined_anchors:0\")),\r\n",
    "    (\"refined_anchors_clipped\", model.ancestor(pillar, \"ROI/refined_anchors_clipped:0\")),\r\n",
    "    (\"post_nms_anchor_ix\", nms_node),\r\n",
    "    (\"proposals\", model.keras_model.get_layer(\"ROI\").output),\r\n",
    "])"
   ],
   "outputs": [],
   "metadata": {
    "id": "RcANFkQLR6hy"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Show top anchors by score (before refinement)\n",
    "limit = 100\n",
    "sorted_anchor_ids = np.argsort(rpn['rpn_class'][:,:,1].flatten())[::-1]\n",
    "visualize.draw_boxes(image, boxes=model.anchors[sorted_anchor_ids[:limit]], ax=get_ax())"
   ],
   "outputs": [],
   "metadata": {
    "id": "IJYPTR47SElV"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Show top anchors with refinement. Then with clipping to image boundaries\n",
    "limit = 50\n",
    "ax = get_ax(1, 2)\n",
    "pre_nms_anchors = utils.denorm_boxes(rpn[\"pre_nms_anchors\"][0], image.shape[:2])\n",
    "refined_anchors = utils.denorm_boxes(rpn[\"refined_anchors\"][0], image.shape[:2])\n",
    "refined_anchors_clipped = utils.denorm_boxes(rpn[\"refined_anchors_clipped\"][0], image.shape[:2])\n",
    "visualize.draw_boxes(image, boxes=pre_nms_anchors[:limit],\n",
    "                     refined_boxes=refined_anchors[:limit], ax=ax[0])\n",
    "visualize.draw_boxes(image, refined_boxes=refined_anchors_clipped[:limit], ax=ax[1])"
   ],
   "outputs": [],
   "metadata": {
    "id": "Jrwa07S_SHWZ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Show refined anchors after non-max suppression\n",
    "limit = 50\n",
    "ixs = rpn[\"post_nms_anchor_ix\"][:limit]\n",
    "visualize.draw_boxes(image, refined_boxes=refined_anchors_clipped[ixs], ax=get_ax())"
   ],
   "outputs": [],
   "metadata": {
    "id": "uWwCWW0DSOF3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Show final proposals\r\n",
    "# These are the same as the previous step (refined anchors \r\n",
    "# after NMS) but with coordinates normalized to [0, 1] range.\r\n",
    "limit = 50\r\n",
    "# Convert back to image coordinates for display\r\n",
    "h, w = config.IMAGE_SHAPE[:2]\r\n",
    "proposals = rpn['proposals'][0, :limit] * np.array([h, w, h, w])\r\n",
    "visualize.draw_boxes(image, refined_boxes=proposals, ax=get_ax())"
   ],
   "outputs": [],
   "metadata": {
    "id": "-ghe0nRHST8E"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PROPOSAL CLASSIFICATION"
   ],
   "metadata": {
    "id": "vF1tRMpXSfmB"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2.a\n"
   ],
   "metadata": {
    "id": "4DgjZrJ5Sk_V"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get input and output to classifier and mask heads.\r\n",
    "mrcnn = model.run_graph([image], [\r\n",
    "    (\"proposals\", model.keras_model.get_layer(\"ROI\").output),\r\n",
    "    (\"probs\", model.keras_model.get_layer(\"mrcnn_class\").output),\r\n",
    "    (\"deltas\", model.keras_model.get_layer(\"mrcnn_bbox\").output),\r\n",
    "    (\"masks\", model.keras_model.get_layer(\"mrcnn_mask\").output),\r\n",
    "    (\"detections\", model.keras_model.get_layer(\"mrcnn_detection\").output),\r\n",
    "])\r\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "z6nzfrzeSZfY"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get detection class IDs. Trim zero padding.\r\n",
    "det_class_ids = mrcnn['detections'][0, :, 4].astype(np.int32)\r\n",
    "det_count = np.where(det_class_ids == 0)[0][0]\r\n",
    "det_class_ids = det_class_ids[:det_count]\r\n",
    "detections = mrcnn['detections'][0, :det_count]\r\n",
    "\r\n",
    "print(\"{} detections: {}\".format(\r\n",
    "    det_count, np.array(dataset.class_names)[det_class_ids]))\r\n",
    "\r\n",
    "captions = [\"{} {:.3f}\".format(dataset.class_names[int(c)], s) if c > 0 else \"\"\r\n",
    "            for c, s in zip(detections[:, 4], detections[:, 5])]\r\n",
    "visualize.draw_boxes(\r\n",
    "    image, \r\n",
    "    refined_boxes=utils.denorm_boxes(detections[:, :4], image.shape[:2]),\r\n",
    "    visibilities=[2] * len(detections),\r\n",
    "    captions=captions, title=\"Detections\",\r\n",
    "    ax=get_ax())"
   ],
   "outputs": [],
   "metadata": {
    "id": "6QLNIyj2SrwX"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Proposals are in normalized coordinates. Scale them\n",
    "# to image coordinates.\n",
    "h, w = config.IMAGE_SHAPE[:2]\n",
    "proposals = np.around(mrcnn[\"proposals\"][0] * np.array([h, w, h, w])).astype(np.int32)\n",
    "\n",
    "# Class ID, score, and mask per proposal\n",
    "roi_class_ids = np.argmax(mrcnn[\"probs\"][0], axis=1)\n",
    "roi_scores = mrcnn[\"probs\"][0, np.arange(roi_class_ids.shape[0]), roi_class_ids]\n",
    "roi_class_names = np.array(dataset.class_names)[roi_class_ids]\n",
    "roi_positive_ixs = np.where(roi_class_ids > 0)[0]\n",
    "\n",
    "# How many ROIs vs empty rows?\n",
    "print(\"{} Valid proposals out of {}\".format(np.sum(np.any(proposals, axis=1)), proposals.shape[0]))\n",
    "print(\"{} Positive ROIs\".format(len(roi_positive_ixs)))\n",
    "\n",
    "# Class counts\n",
    "print(list(zip(*np.unique(roi_class_names, return_counts=True))))"
   ],
   "outputs": [],
   "metadata": {
    "id": "hic7gWUNSvTl"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Display a random sample of proposals.\r\n",
    "# Proposals classified as background are dotted, and\r\n",
    "# the rest show their class and confidence score.\r\n",
    "limit = 200\r\n",
    "ixs = np.random.randint(0, proposals.shape[0], limit)\r\n",
    "captions = [\"{} {:.3f}\".format(dataset.class_names[c], s) if c > 0 else \"\"\r\n",
    "            for c, s in zip(roi_class_ids[ixs], roi_scores[ixs])]\r\n",
    "visualize.draw_boxes(image, boxes=proposals[ixs],\r\n",
    "                     visibilities=np.where(roi_class_ids[ixs] > 0, 2, 1),\r\n",
    "                     captions=captions, title=\"ROIs Before Refinement\",\r\n",
    "                     ax=get_ax())"
   ],
   "outputs": [],
   "metadata": {
    "id": "Fp8gc5rmTNrJ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Class-specific bounding box shifts.\r\n",
    "roi_bbox_specific = mrcnn[\"deltas\"][0, np.arange(proposals.shape[0]), roi_class_ids]\r\n",
    "log(\"roi_bbox_specific\", roi_bbox_specific)\r\n",
    "\r\n",
    "# Apply bounding box transformations\r\n",
    "# Shape: [N, (y1, x1, y2, x2)]\r\n",
    "refined_proposals = utils.apply_box_deltas(\r\n",
    "    proposals, roi_bbox_specific * config.BBOX_STD_DEV).astype(np.int32)\r\n",
    "log(\"refined_proposals\", refined_proposals)\r\n",
    "\r\n",
    "# Show positive proposals\r\n",
    "ids = np.arange(r['rois'].shape[0])  # Display all\r\n",
    "limit = 5\r\n",
    "ids = np.random.randint(0, len(roi_positive_ixs), limit)  # Display random sample\r\n",
    "captions = [\"{} {:.3f}\".format(dataset.class_names[c], s) if c > 0 else \"\"\r\n",
    "            for c, s in zip(roi_class_ids[roi_positive_ixs][ids], roi_scores[roi_positive_ixs][ids])]\r\n",
    "visualize.draw_boxes(image, boxes=proposals[roi_positive_ixs][ids],\r\n",
    "                     refined_boxes=refined_proposals[roi_positive_ixs][ids],\r\n",
    "                     visibilities=np.where(roi_class_ids[roi_positive_ixs][ids] > 0, 1, 0),\r\n",
    "                     captions=captions, title=\"ROIs After Refinement\",\r\n",
    "                     ax=get_ax())"
   ],
   "outputs": [],
   "metadata": {
    "id": "JWzVhV6qTSp_"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Remove boxes classified as background\r\n",
    "keep = np.where(roi_class_ids > 0)[0]\r\n",
    "print(\"Keep {} detections:\\n{}\".format(keep.shape[0], keep))"
   ],
   "outputs": [],
   "metadata": {
    "id": "YrC2ysonTd0m"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Remove low confidence detections\r\n",
    "keep = np.intersect1d(keep, np.where(roi_scores >= config.DETECTION_MIN_CONFIDENCE)[0])\r\n",
    "print(\"Remove boxes below {} confidence. Keep {}:\\n{}\".format(\r\n",
    "    config.DETECTION_MIN_CONFIDENCE, keep.shape[0], keep))\r\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "GUwnTi_3Tl_D"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Apply per-class non-max suppression\r\n",
    "pre_nms_boxes = refined_proposals[keep]\r\n",
    "pre_nms_scores = roi_scores[keep]\r\n",
    "pre_nms_class_ids = roi_class_ids[keep]\r\n",
    "\r\n",
    "nms_keep = []\r\n",
    "for class_id in np.unique(pre_nms_class_ids):\r\n",
    "    # Pick detections of this class\r\n",
    "    ixs = np.where(pre_nms_class_ids == class_id)[0]\r\n",
    "    # Apply NMS\r\n",
    "    class_keep = utils.non_max_suppression(pre_nms_boxes[ixs], \r\n",
    "                                            pre_nms_scores[ixs],\r\n",
    "                                            config.DETECTION_NMS_THRESHOLD)\r\n",
    "    # Map indicies\r\n",
    "    class_keep = keep[ixs[class_keep]]\r\n",
    "    nms_keep = np.union1d(nms_keep, class_keep)\r\n",
    "    print(\"{:22}: {} -> {}\".format(dataset.class_names[class_id][:20], \r\n",
    "                                   keep[ixs], class_keep))\r\n",
    "\r\n",
    "keep = np.intersect1d(keep, nms_keep).astype(np.int32)\r\n",
    "print(\"\\nKept after per-class NMS: {}\\n{}\".format(keep.shape[0], keep))"
   ],
   "outputs": [],
   "metadata": {
    "id": "ONHcZH8CTn7B"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Show final detections\r\n",
    "ixs = np.arange(len(keep))  # Display all\r\n",
    "# ixs = np.random.randint(0, len(keep), 10)  # Display random sample\r\n",
    "captions = [\"{} {:.3f}\".format(dataset.class_names[c], s) if c > 0 else \"\"\r\n",
    "            for c, s in zip(roi_class_ids[keep][ixs], roi_scores[keep][ixs])]\r\n",
    "visualize.draw_boxes(\r\n",
    "    image, boxes=proposals[keep][ixs],\r\n",
    "    refined_boxes=refined_proposals[keep][ixs],\r\n",
    "    visibilities=np.where(roi_class_ids[keep][ixs] > 0, 1, 0),\r\n",
    "    captions=captions, title=\"Detections after NMS\",\r\n",
    "    ax=get_ax())"
   ],
   "outputs": [],
   "metadata": {
    "id": "Ef-bnf2ATsJa"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GENERATING MASKS\n",
    "3.a\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "lNbNjdOAUgDn"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "display_images(np.transpose(gt_mask, [2, 0, 1]), cmap=\"Blues\")"
   ],
   "outputs": [],
   "metadata": {
    "id": "gDI_ptMgUYtD"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get predictions of mask head\r\n",
    "mrcnn = model.run_graph([image], [\r\n",
    "    (\"detections\", model.keras_model.get_layer(\"mrcnn_detection\").output),\r\n",
    "    (\"masks\", model.keras_model.get_layer(\"mrcnn_mask\").output),\r\n",
    "])\r\n",
    "\r\n",
    "# Get detection class IDs. Trim zero padding.\r\n",
    "det_class_ids = mrcnn['detections'][0, :, 4].astype(np.int32)\r\n",
    "det_count = np.where(det_class_ids == 0)[0][0]\r\n",
    "det_class_ids = det_class_ids[:det_count]\r\n",
    "\r\n",
    "print(\"{} detections: {}\".format(\r\n",
    "    det_count, np.array(dataset.class_names)[det_class_ids]))"
   ],
   "outputs": [],
   "metadata": {
    "id": "ZM_f1-h_UoTN"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Masks\r\n",
    "det_boxes = utils.denorm_boxes(mrcnn[\"detections\"][0, :, :4], image.shape[:2])\r\n",
    "det_mask_specific = np.array([mrcnn[\"masks\"][0, i, :, :, c] \r\n",
    "                              for i, c in enumerate(det_class_ids)])\r\n",
    "det_masks = np.array([utils.unmold_mask(m, det_boxes[i], image.shape)\r\n",
    "                      for i, m in enumerate(det_mask_specific)])\r\n",
    "log(\"det_mask_specific\", det_mask_specific)\r\n",
    "log(\"det_masks\", det_masks)"
   ],
   "outputs": [],
   "metadata": {
    "id": "5eQp7FVcUrE8"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "display_images(det_mask_specific[:4] * 255, cmap=\"Blues\", interpolation=\"none\")"
   ],
   "outputs": [],
   "metadata": {
    "id": "JLUQc7tBUvpd"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "display_images(det_masks[:4] * 255, cmap=\"Blues\", interpolation=\"none\")\r\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "BQu4KJCCUzK-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualize Activations"
   ],
   "metadata": {
    "id": "SIdut37OU53t"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get activations of a few sample layers\r\n",
    "activations = model.run_graph([image], [\r\n",
    "    (\"input_image\",        tf.identity(model.keras_model.get_layer(\"input_image\").output)),\r\n",
    "    (\"res2c_out\",          model.keras_model.get_layer(\"res2c_out\").output),\r\n",
    "    (\"res3c_out\",          model.keras_model.get_layer(\"res3c_out\").output),\r\n",
    "    (\"res4w_out\",          model.keras_model.get_layer(\"res4w_out\").output),  # for resnet100\r\n",
    "    (\"rpn_bbox\",           model.keras_model.get_layer(\"rpn_bbox\").output),\r\n",
    "    (\"roi\",                model.keras_model.get_layer(\"ROI\").output),\r\n",
    "])"
   ],
   "outputs": [],
   "metadata": {
    "id": "u4afyvo_U1Wu"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Input image (normalized)\r\n",
    "_ = plt.imshow(modellib.unmold_image(activations[\"input_image\"][0],config))\r\n"
   ],
   "outputs": [],
   "metadata": {
    "id": "UbNpV-NKVA6U"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Backbone feature map\r\n",
    "display_images(np.transpose(activations[\"res2c_out\"][0,:,:,:4], [2, 0, 1]), cols=4)"
   ],
   "outputs": [],
   "metadata": {
    "id": "VfP4v70WVD3c"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {
    "id": "awaGgdIDVKLE"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {
    "id": "-3ey4tZqAl34"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**JSON FORMATTER**"
   ],
   "metadata": {
    "id": "EfPTWeYcC-qM"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\r\n",
    "import os\r\n",
    "\r\n",
    "#change the Json path\r\n",
    "file = \"/content/drive/MyDrive/dataset2/train/WHO.json\"\r\n",
    "annotations = json.load(open(file))\r\n",
    "\r\n",
    "change = {}\r\n",
    "for i,a in enumerate(annotations1):\r\n",
    "  #change the WHO accordingly\r\n",
    "  annotations[a][\"filename\"] = \"WHO_{}.jpg\".format(i)\r\n",
    "  change[a] = annotations[a][\"filename\"]\r\n",
    "\r\n",
    "def updateJsonFile():\r\n",
    "    jsonFile = open(file, \"r\") # Open the JSON file for reading\r\n",
    "    data = json.load(jsonFile) # Read the JSON into the buffer\r\n",
    "    jsonFile.close() # Close the JSON file\r\n",
    "\r\n",
    "    ## Save our changes to JSON file\r\n",
    "    jsonFile = open(file, \"w+\")\r\n",
    "    jsonFile.write(json.dumps(annotations))\r\n",
    "    jsonFile.close()\r\n",
    "\r\n",
    "print(\"Over writing JSON file\")\r\n",
    "updateJsonFile()\r\n",
    "\r\n",
    "#Change the path to the exact folders that you are working on\r\n",
    "for root, dirs, files in os.walk(\"/content/drive/MyDrive/dataset2/train\"):\r\n",
    "    for i,f in enumerate(files):\r\n",
    "        absname = os.path.join(root, f)\r\n",
    "        if f.endswith(\".jpg\"):\r\n",
    "          newname = os.path.join(root, (change[f].jpg))\r\n",
    "        elif f.endswith(\".png\"):\r\n",
    "          newname = os.path.join(root, (change[f].png))\r\n",
    "        elif f.endswith(\".jpeg\"):\r\n",
    "          newname = os.path.join(root, (change[f].jgeg))\r\n",
    "        else:\r\n",
    "          newname = absname\r\n",
    "        os.rename(absname, newname)\r\n",
    "\r\n",
    "print(\"DONE!!! You can check your drive for updated filenames\")"
   ],
   "outputs": [],
   "metadata": {
    "id": "fpbtL9Ux-YNn"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if class_id == 1: \r\n",
    "            masked_image = apply_mask(masked_image, mask, [1, 0, 0], alpha=1)\r\n",
    "          elif class_id == 2:\r\n",
    "            masked_image = apply_mask(masked_image, mask, [0, 1, 0], alpha=1)\r\n",
    "          elif class_id == 3:\r\n",
    "            masked_image = apply_mask(masked_image, mask, [0, 0, 1], alpha=1)\r\n",
    "          elif class_id == 4:\r\n",
    "            masked_image = apply_mask(masked_image, mask, [1, 0, 1], alpha=1)\r\n",
    "          elif class_id == 5:\r\n",
    "            masked_image = apply_mask(masked_image, mask, [0, 1, 1], alpha=1)\r\n",
    "          elif class_id == 6:\r\n",
    "            masked_image = apply_mask(masked_image, mask, [1, 1, 0], alpha=1)"
   ],
   "outputs": [],
   "metadata": {
    "id": "m_DenrRm5ztp"
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "ER2OFIbzFrhw",
    "4ANrZQQVRtdA",
    "XwD8dV63SCOE",
    "vF1tRMpXSfmB"
   ],
   "name": "Seg (2).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}